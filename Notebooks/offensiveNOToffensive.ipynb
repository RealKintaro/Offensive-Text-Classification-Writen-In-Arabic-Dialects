{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"DOCaTMFLZZpC"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nimport random\nimport time\nimport gc\nimport pickle\n\nimport nltk\nnltk.download('stopwords')\nimport emoji\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:25.093406Z","iopub.execute_input":"2022-12-12T03:06:25.094641Z","iopub.status.idle":"2022-12-12T03:06:28.614473Z","shell.execute_reply.started":"2022-12-12T03:06:25.094557Z","shell.execute_reply":"2022-12-12T03:06:28.613408Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:31.608368Z","iopub.execute_input":"2022-12-12T03:06:31.608805Z","iopub.status.idle":"2022-12-12T03:06:31.694821Z","shell.execute_reply.started":"2022-12-12T03:06:31.608770Z","shell.execute_reply":"2022-12-12T03:06:31.693581Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(torch.cuda.get_device_name(device))","metadata":{"id":"cKNYp_ZoN8Jq","outputId":"4a99b4b2-32e0-49a3-8a3a-50265528c7f2","execution":{"iopub.status.busy":"2022-12-12T03:06:32.693953Z","iopub.execute_input":"2022-12-12T03:06:32.694365Z","iopub.status.idle":"2022-12-12T03:06:32.702450Z","shell.execute_reply.started":"2022-12-12T03:06:32.694329Z","shell.execute_reply":"2022-12-12T03:06:32.701165Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preparation and cleaning\n","metadata":{"id":"oYZxEOZ0ZUsA"}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/final_dataset.csv', encoding='utf-16')\n#df.columns = ['Comment', 'Majority_Label']\ndf.head()\n","metadata":{"id":"Z3S7hzZfOASV","outputId":"28d27939-2f96-45f3-b000-60d72a133842","execution":{"iopub.status.busy":"2022-12-12T03:06:35.545695Z","iopub.execute_input":"2022-12-12T03:06:35.546656Z","iopub.status.idle":"2022-12-12T03:06:35.645801Z","shell.execute_reply.started":"2022-12-12T03:06:35.546606Z","shell.execute_reply":"2022-12-12T03:06:35.644636Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                             Comment  Majority_Label\n0  ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠÙ‡ Ø§Ù„Ù„Ø¨Ù†Ø§Ù†ÙŠ Ø¬Ø¨Ø±Ø§Ù† Ø¨Ø§Ø³ÙŠÙ„ Ù‚Ø§Ù„ Ø³Ù„Ø³Ù„Ù‡ Øª...             0.0\n1          Ø³ÙˆØ±ÙŠÙ‡ Ø¨Ù„Ø¯ Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª ØªØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø¹Ù„ÙŠÙ‡ Ø§Ùˆ Ø¨Ø­ÙŠÙˆØ§Ù†             0.0\n2  Ø§Ø®ÙŠ Ø§Ù„Ø­Ø§Ø¬ Ø§Ø°Ø§ Ø´Ø¹Ø±Øª Ø§Ù†Ùƒ Ù…Ø­Ø±Ø¬Ø§ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ø¯Ø§Øª Ù„ØªØµØ±ÙŠØ­...             0.0\n3  ÙÙŠÙƒ ØªØ¹ÙŠØ´ Ø¨Ù„Ø§ ØªÙƒØ¨ ÙØªÙ† Ù„ÙŠÙ„ Ù†Ù‡Ø§Ø± ÙˆØ¨ÙƒØ±Ù‡ Ù‚Ù„Ù‡Ù… Ø§Ù„Ù…ÙˆØ¶...             0.0\n4   Ø§Ù„Ø¨Ø·Ù„ Ù‚Ø§ØªÙ„ ÙˆØ¬Ø§Ø°Ù Ø¨Ø­ÙŠØ§ØªÙ‡ Ù„ØªØ­ÙŠØ§ Ø§Ù†Øª ÙˆØ§Ø·ÙŠ Ø¹ÙŠØ¨ Ø§Ù„Ø´ÙˆÙ…             1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Majority_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ÙˆØ²ÙŠØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠÙ‡ Ø§Ù„Ù„Ø¨Ù†Ø§Ù†ÙŠ Ø¬Ø¨Ø±Ø§Ù† Ø¨Ø§Ø³ÙŠÙ„ Ù‚Ø§Ù„ Ø³Ù„Ø³Ù„Ù‡ Øª...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ø³ÙˆØ±ÙŠÙ‡ Ø¨Ù„Ø¯ Ø§Ù„Ø­Ø¶Ø§Ø±Ø§Øª ØªØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø¹Ù„ÙŠÙ‡ Ø§Ùˆ Ø¨Ø­ÙŠÙˆØ§Ù†</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ø§Ø®ÙŠ Ø§Ù„Ø­Ø§Ø¬ Ø§Ø°Ø§ Ø´Ø¹Ø±Øª Ø§Ù†Ùƒ Ù…Ø­Ø±Ø¬Ø§ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ø¯Ø§Øª Ù„ØªØµØ±ÙŠØ­...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ÙÙŠÙƒ ØªØ¹ÙŠØ´ Ø¨Ù„Ø§ ØªÙƒØ¨ ÙØªÙ† Ù„ÙŠÙ„ Ù†Ù‡Ø§Ø± ÙˆØ¨ÙƒØ±Ù‡ Ù‚Ù„Ù‡Ù… Ø§Ù„Ù…ÙˆØ¶...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ø§Ù„Ø¨Ø·Ù„ Ù‚Ø§ØªÙ„ ÙˆØ¬Ø§Ø°Ù Ø¨Ø­ÙŠØ§ØªÙ‡ Ù„ØªØ­ÙŠØ§ Ø§Ù†Øª ÙˆØ§Ø·ÙŠ Ø¹ÙŠØ¨ Ø§Ù„Ø´ÙˆÙ…</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape[0] - df.dropna().shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:46.701298Z","iopub.execute_input":"2022-12-12T03:06:46.702391Z","iopub.status.idle":"2022-12-12T03:06:46.722735Z","shell.execute_reply.started":"2022-12-12T03:06:46.702351Z","shell.execute_reply":"2022-12-12T03:06:46.721806Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"df=df.dropna()\n# sorting by first name\ndf.sort_values(\"Comment\", inplace=True)\n# dropping ALL duplicate values\ndf.drop_duplicates(subset=\"Comment\",\n                     keep=False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:49.925195Z","iopub.execute_input":"2022-12-12T03:06:49.925594Z","iopub.status.idle":"2022-12-12T03:06:49.992137Z","shell.execute_reply.started":"2022-12-12T03:06:49.925563Z","shell.execute_reply":"2022-12-12T03:06:49.991166Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"arabic_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n\narabic_diacritics = re.compile(\"\"\"\n                             Ù‘    | # Tashdid\n                             Ù    | # Fatha\n                             Ù‹    | # Tanwin Fath\n                             Ù    | # Damma\n                             ÙŒ    | # Tanwin Damm\n                             Ù    | # Kasra\n                             Ù    | # Tanwin Kasr\n                             Ù’    | # Sukun\n                             Ù€     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n\narabic_punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''\nenglish_punctuations = string.punctuation\npunctuations = arabic_punctuations + english_punctuations\n\n\ndef remove_urls (text):\n    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    return text\n\n\ndef remove_emails(text):\n    text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", \"\",  text, flags=re.MULTILINE)\n    return text\n\n# def remove_emoji(text):\n#     return emoji.get_emoji_regexp().sub(u'', text)\n\ndef remove_emoji(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\ndef normalization(text):\n    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n    text = re.sub(\"Ø©\", \"Ù‡\", text)\n    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n    return text\n\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', text)\n    return text\n\ndef remove_stopwords(text):\n    filtered_sentence = [w for w in text.split() if not w in arabic_stopwords]\n    return ' '.join(filtered_sentence)\n\ndef cleaning_content(line):\n    if (isinstance(line, float)):\n        return None\n    line.replace('\\n', ' ')\n    line = remove_emails(line)\n    line = remove_urls(line)\n    line = remove_emoji(line)\n    nline = [w if '@' not in w else 'USERID' for w in line.split()]\n    line = ' '.join(nline)\n    line = line.replace('RT', '').replace('<LF>', '').replace('<br />','').replace('&quot;', '').replace('<url>', '').replace('USERID', '')\n\n\n    # add spaces between punc,\n    line = line.translate(str.maketrans({key: \" {0} \".format(key) for key in punctuations}))\n\n    # then remove punc,\n    translator = str.maketrans('', '', punctuations)\n    line = line.translate(translator)\n\n    line = remove_stopwords(line)\n    line=remove_diacritics(normalization(line))\n    return line\n\ndef hasDigits(s):\n    return any( 48 <= ord(char) <= 57  or 1632 <= ord(char) <= 1641 for char in s)\n","metadata":{"id":"G8qBDZAUOvOz","execution":{"iopub.status.busy":"2022-12-12T03:06:51.693932Z","iopub.execute_input":"2022-12-12T03:06:51.694365Z","iopub.status.idle":"2022-12-12T03:06:51.714678Z","shell.execute_reply.started":"2022-12-12T03:06:51.694330Z","shell.execute_reply":"2022-12-12T03:06:51.713624Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.Comment = df.Comment.apply(cleaning_content)","metadata":{"id":"C9cVYayEQKP_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments = ' '.join(list(df.Comment))\nwords = comments.split(' ')","metadata":{"id":"1k6n4gnAQKJm","execution":{"iopub.status.busy":"2022-12-12T03:06:55.297588Z","iopub.execute_input":"2022-12-12T03:06:55.297966Z","iopub.status.idle":"2022-12-12T03:06:55.352762Z","shell.execute_reply.started":"2022-12-12T03:06:55.297934Z","shell.execute_reply":"2022-12-12T03:06:55.351788Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(df.Comment)\ncorpus[:10]","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:06:58.632660Z","iopub.execute_input":"2022-12-12T03:06:58.633069Z","iopub.status.idle":"2022-12-12T03:06:58.738203Z","shell.execute_reply.started":"2022-12-12T03:06:58.633037Z","shell.execute_reply":"2022-12-12T03:06:58.737124Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['<url>',\n 'Ø¥Ù„Ù‰',\n 'Ø§ØªØ®Ø¶',\n 'Ø²Ù‰',\n 'Ù„Ø§ÙŠÙƒğŸ˜‚ğŸ˜‚',\n '<url>',\n 'Ø§Ù„Ù„Ù‡',\n 'ØºØ§Ù„Ø¨Ù…Ù†',\n 'Ø§Ù„Ø§Ø±Ø¯Ù†',\n 'Ù„Ù„ÙˆØ·Ù†']"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:07:00.722606Z","iopub.execute_input":"2022-12-12T03:07:00.723145Z","iopub.status.idle":"2022-12-12T03:07:00.814929Z","shell.execute_reply.started":"2022-12-12T03:07:00.723111Z","shell.execute_reply":"2022-12-12T03:07:00.814017Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'Ø§Ù„Ù„Ù‡': 4602,\n 'Ù…Ù†': 4175,\n 'Ùˆ': 3567,\n 'ÙÙŠ': 3082,\n 'Ø¹Ù„Ù‰': 2426,\n 'Ù„Ø§': 1471,\n 'Ù…Ø§': 1121,\n 'Ø§Ù†': 1066,\n 'ÙŠØ§': 1032,\n 'Ø§Ù†Øª': 982}"},"metadata":{}}]},{"cell_type":"code","source":"X = df.Comment.values\nY = df.Majority_Label.values\n\nX_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.1, random_state = random.seed(42))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:07:10.109871Z","iopub.execute_input":"2022-12-12T03:07:10.110263Z","iopub.status.idle":"2022-12-12T03:07:10.118045Z","shell.execute_reply.started":"2022-12-12T03:07:10.110207Z","shell.execute_reply":"2022-12-12T03:07:10.116933Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02', do_lower_case=True)\n\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    input_ids = []\n    attention_masks = []\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=cleaning_content(sent),  \n            add_special_tokens=True,        \n            max_length=MAX_LEN,             \n            pad_to_max_length=True,         \n            return_attention_mask=True      \n        )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:07:17.946958Z","iopub.execute_input":"2022-12-12T03:07:17.947469Z","iopub.status.idle":"2022-12-12T03:07:18.931586Z","shell.execute_reply.started":"2022-12-12T03:07:17.947434Z","shell.execute_reply":"2022-12-12T03:07:18.930601Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MAX_LEN =  256\n\ntoken_ids = list(preprocessing_for_bert([X_train[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"id":"5tfA9o2URljj","outputId":"35097edf-cd0c-46f0-b4f4-d64ef48134da","execution":{"iopub.status.busy":"2022-12-12T03:07:22.627066Z","iopub.execute_input":"2022-12-12T03:07:22.627719Z","iopub.status.idle":"2022-12-12T03:07:39.601854Z","shell.execute_reply.started":"2022-12-12T03:07:22.627680Z","shell.execute_reply":"2022-12-12T03:07:39.600806Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Original:   <url> Ø¥Ù„Ù‰ Ø§ØªØ®Ø¶ Ø²Ù‰ Ù„Ø§ÙŠÙƒğŸ˜‚ğŸ˜‚ \nToken IDs:  [2, 7252, 195, 678, 50261, 181, 39605, 7375, 1330, 1449, 21830, 15704, 2396, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nTokenizing data...\n","output_type":"stream"}]},{"cell_type":"code","source":"Y_train","metadata":{"id":"WL0IIKADR7ft","outputId":"65e495c8-cd08-4c41-c9b8-b7edf3ecc316","execution":{"iopub.status.busy":"2022-12-12T03:08:24.335953Z","iopub.execute_input":"2022-12-12T03:08:24.336373Z","iopub.status.idle":"2022-12-12T03:08:24.344173Z","shell.execute_reply.started":"2022-12-12T03:08:24.336337Z","shell.execute_reply":"2022-12-12T03:08:24.342899Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([1., 1., 1., ..., 0., 1., 0.])"},"metadata":{}}]},{"cell_type":"code","source":"train_labels = torch.tensor(Y_train.astype(float))\nval_labels = torch.tensor(Y_val.astype(float))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:08:26.366537Z","iopub.execute_input":"2022-12-12T03:08:26.367549Z","iopub.status.idle":"2022-12-12T03:08:26.373803Z","shell.execute_reply.started":"2022-12-12T03:08:26.367500Z","shell.execute_reply":"2022-12-12T03:08:26.372466Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:08:57.209107Z","iopub.execute_input":"2022-12-12T03:08:57.209520Z","iopub.status.idle":"2022-12-12T03:08:57.217540Z","shell.execute_reply.started":"2022-12-12T03:08:57.209487Z","shell.execute_reply":"2022-12-12T03:08:57.216204Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\n\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        #  hidden size of BERT, hidden size of our classifier, number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('aubmindlab/bert-base-arabertv02')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)  \n        \n        # Extract the last hidden state of the token `[CLS]` for classification task and feed them to classifier to compute logits \n        last_hidden_state_cls = outputs[0][:, 0, :]\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"id":"9DSY4rPeSD-A","outputId":"717f451f-6569-4317-9e41-a71cceccd3b4","execution":{"iopub.status.busy":"2022-12-12T03:08:59.855854Z","iopub.execute_input":"2022-12-12T03:08:59.856246Z","iopub.status.idle":"2022-12-12T03:08:59.865330Z","shell.execute_reply.started":"2022-12-12T03:08:59.856200Z","shell.execute_reply":"2022-12-12T03:08:59.864266Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"CPU times: user 26 Âµs, sys: 1 Âµs, total: 27 Âµs\nWall time: 31 Âµs\n","output_type":"stream"}]},{"cell_type":"code","source":"def initialize_model(epochs=3):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    bert_classifier.to(device)\n\n    optimizer = torch.optim.AdamW(bert_classifier.parameters(),lr=0.0001,eps=1e-8)\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"id":"iDyHb2_zSFzn","execution":{"iopub.status.busy":"2022-12-12T03:09:02.160714Z","iopub.execute_input":"2022-12-12T03:09:02.161093Z","iopub.status.idle":"2022-12-12T03:09:02.169197Z","shell.execute_reply.started":"2022-12-12T03:09:02.161061Z","shell.execute_reply":"2022-12-12T03:09:02.168202Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n            \n            # Always clear any previously calculated gradients before performing a\n            # backward pass. PyTorch doesn't do this automatically because \n            # accumulating the gradients is \"convenient while training RN\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n            \n            # Accumulate the training loss over all of the batches so that we can\n            # calculate the average loss at the end. `loss` is a Tensor containing a\n            # single value; the `.item()` function just returns the Python value \n            # from the tensor.\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels.long())\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n        \n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n        \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels.long())\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-12-12T03:09:05.008157Z","iopub.execute_input":"2022-12-12T03:09:05.008766Z","iopub.status.idle":"2022-12-12T03:09:05.028650Z","shell.execute_reply.started":"2022-12-12T03:09:05.008731Z","shell.execute_reply":"2022-12-12T03:09:05.027685Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()   \n\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=5)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)","metadata":{"id":"8WXZh9TdSLkz","outputId":"05c4a182-7f5d-4d1d-a1a3-074f1296711b","execution":{"iopub.status.busy":"2022-12-12T03:09:12.559440Z","iopub.execute_input":"2022-12-12T03:09:12.559854Z","iopub.status.idle":"2022-12-12T04:18:03.516320Z","shell.execute_reply.started":"2022-12-12T03:09:12.559822Z","shell.execute_reply":"2022-12-12T04:18:03.515187Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Start training...\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   1    |   20    |   0.632697   |     -      |     -     |   25.70  \n   1    |   40    |   0.575155   |     -      |     -     |   24.67  \n   1    |   60    |   0.567682   |     -      |     -     |   25.44  \n   1    |   80    |   0.532807   |     -      |     -     |   26.46  \n   1    |   100   |   0.530171   |     -      |     -     |   26.30  \n   1    |   120   |   0.532441   |     -      |     -     |   25.95  \n   1    |   140   |   0.483929   |     -      |     -     |   26.07  \n   1    |   160   |   0.501306   |     -      |     -     |   26.23  \n   1    |   180   |   0.459289   |     -      |     -     |   26.19  \n   1    |   200   |   0.480510   |     -      |     -     |   26.18  \n   1    |   220   |   0.493092   |     -      |     -     |   26.17  \n   1    |   240   |   0.441885   |     -      |     -     |   26.17  \n   1    |   260   |   0.440011   |     -      |     -     |   26.15  \n   1    |   280   |   0.453355   |     -      |     -     |   26.15  \n   1    |   300   |   0.469530   |     -      |     -     |   26.16  \n   1    |   320   |   0.444506   |     -      |     -     |   26.16  \n   1    |   340   |   0.438919   |     -      |     -     |   26.15  \n   1    |   360   |   0.431392   |     -      |     -     |   26.14  \n   1    |   380   |   0.407238   |     -      |     -     |   26.15  \n   1    |   400   |   0.406337   |     -      |     -     |   26.13  \n   1    |   420   |   0.462693   |     -      |     -     |   26.15  \n   1    |   440   |   0.455140   |     -      |     -     |   26.12  \n   1    |   460   |   0.399589   |     -      |     -     |   26.14  \n   1    |   480   |   0.416999   |     -      |     -     |   26.12  \n   1    |   500   |   0.427660   |     -      |     -     |   26.12  \n   1    |   520   |   0.422805   |     -      |     -     |   26.12  \n   1    |   540   |   0.425475   |     -      |     -     |   26.13  \n   1    |   560   |   0.437308   |     -      |     -     |   26.14  \n   1    |   580   |   0.443980   |     -      |     -     |   26.15  \n   1    |   600   |   0.417805   |     -      |     -     |   26.15  \n   1    |   606   |   0.457651   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   1    |    -    |   0.467896   |  0.381081  |   82.69   |  822.69  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   2    |   20    |   0.275117   |     -      |     -     |   27.56  \n   2    |   40    |   0.295583   |     -      |     -     |   26.20  \n   2    |   60    |   0.248752   |     -      |     -     |   26.15  \n   2    |   80    |   0.325117   |     -      |     -     |   26.20  \n   2    |   100   |   0.304514   |     -      |     -     |   26.22  \n   2    |   120   |   0.289015   |     -      |     -     |   26.16  \n   2    |   140   |   0.352266   |     -      |     -     |   26.13  \n   2    |   160   |   0.237176   |     -      |     -     |   26.12  \n   2    |   180   |   0.285141   |     -      |     -     |   26.12  \n   2    |   200   |   0.257377   |     -      |     -     |   26.06  \n   2    |   220   |   0.279229   |     -      |     -     |   26.08  \n   2    |   240   |   0.308591   |     -      |     -     |   26.13  \n   2    |   260   |   0.316307   |     -      |     -     |   26.10  \n   2    |   280   |   0.263285   |     -      |     -     |   26.13  \n   2    |   300   |   0.279234   |     -      |     -     |   26.12  \n   2    |   320   |   0.273642   |     -      |     -     |   26.12  \n   2    |   340   |   0.269718   |     -      |     -     |   26.09  \n   2    |   360   |   0.289498   |     -      |     -     |   26.11  \n   2    |   380   |   0.256761   |     -      |     -     |   26.10  \n   2    |   400   |   0.266357   |     -      |     -     |   26.12  \n   2    |   420   |   0.253105   |     -      |     -     |   26.15  \n   2    |   440   |   0.271702   |     -      |     -     |   26.14  \n   2    |   460   |   0.265906   |     -      |     -     |   26.15  \n   2    |   480   |   0.266736   |     -      |     -     |   26.13  \n   2    |   500   |   0.279243   |     -      |     -     |   26.16  \n   2    |   520   |   0.268492   |     -      |     -     |   26.16  \n   2    |   540   |   0.268468   |     -      |     -     |   26.14  \n   2    |   560   |   0.283312   |     -      |     -     |   26.06  \n   2    |   580   |   0.289147   |     -      |     -     |   26.21  \n   2    |   600   |   0.256276   |     -      |     -     |   26.25  \n   2    |   606   |   0.249149   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   2    |    -    |   0.278866   |  0.375432  |   84.44   |  826.16  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   3    |   20    |   0.131200   |     -      |     -     |   27.48  \n   3    |   40    |   0.158962   |     -      |     -     |   26.24  \n   3    |   60    |   0.148332   |     -      |     -     |   26.13  \n   3    |   80    |   0.163261   |     -      |     -     |   26.09  \n   3    |   100   |   0.118509   |     -      |     -     |   26.06  \n   3    |   120   |   0.215144   |     -      |     -     |   26.08  \n   3    |   140   |   0.185493   |     -      |     -     |   26.12  \n   3    |   160   |   0.164525   |     -      |     -     |   26.13  \n   3    |   180   |   0.128894   |     -      |     -     |   26.14  \n   3    |   200   |   0.207446   |     -      |     -     |   26.17  \n   3    |   220   |   0.169355   |     -      |     -     |   26.14  \n   3    |   240   |   0.152911   |     -      |     -     |   26.17  \n   3    |   260   |   0.177090   |     -      |     -     |   26.19  \n   3    |   280   |   0.170668   |     -      |     -     |   26.14  \n   3    |   300   |   0.135046   |     -      |     -     |   26.11  \n   3    |   320   |   0.120991   |     -      |     -     |   26.09  \n   3    |   340   |   0.111832   |     -      |     -     |   26.07  \n   3    |   360   |   0.132593   |     -      |     -     |   26.12  \n   3    |   380   |   0.190670   |     -      |     -     |   26.13  \n   3    |   400   |   0.116443   |     -      |     -     |   26.09  \n   3    |   420   |   0.145945   |     -      |     -     |   26.13  \n   3    |   440   |   0.153953   |     -      |     -     |   26.10  \n   3    |   460   |   0.167467   |     -      |     -     |   26.11  \n   3    |   480   |   0.154166   |     -      |     -     |   26.09  \n   3    |   500   |   0.172910   |     -      |     -     |   26.07  \n   3    |   520   |   0.145143   |     -      |     -     |   26.03  \n   3    |   540   |   0.137809   |     -      |     -     |   26.15  \n   3    |   560   |   0.134882   |     -      |     -     |   26.21  \n   3    |   580   |   0.212680   |     -      |     -     |   26.22  \n   3    |   600   |   0.164158   |     -      |     -     |   26.20  \n   3    |   606   |   0.111170   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   3    |    -    |   0.155795   |  0.448201  |   84.76   |  825.79  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   4    |   20    |   0.079654   |     -      |     -     |   27.42  \n   4    |   40    |   0.103250   |     -      |     -     |   26.20  \n   4    |   60    |   0.074128   |     -      |     -     |   26.22  \n   4    |   80    |   0.082321   |     -      |     -     |   26.09  \n   4    |   100   |   0.050973   |     -      |     -     |   26.03  \n   4    |   120   |   0.030560   |     -      |     -     |   26.11  \n   4    |   140   |   0.057613   |     -      |     -     |   26.15  \n   4    |   160   |   0.063887   |     -      |     -     |   26.16  \n   4    |   180   |   0.110825   |     -      |     -     |   26.17  \n   4    |   200   |   0.114845   |     -      |     -     |   26.17  \n   4    |   220   |   0.054168   |     -      |     -     |   26.15  \n   4    |   240   |   0.070327   |     -      |     -     |   26.15  \n   4    |   260   |   0.102828   |     -      |     -     |   26.17  \n   4    |   280   |   0.064933   |     -      |     -     |   26.11  \n   4    |   300   |   0.065186   |     -      |     -     |   26.07  \n   4    |   320   |   0.124393   |     -      |     -     |   26.08  \n   4    |   340   |   0.078961   |     -      |     -     |   26.09  \n   4    |   360   |   0.092712   |     -      |     -     |   26.10  \n   4    |   380   |   0.074000   |     -      |     -     |   26.06  \n   4    |   400   |   0.110029   |     -      |     -     |   26.07  \n   4    |   420   |   0.094902   |     -      |     -     |   26.11  \n   4    |   440   |   0.080456   |     -      |     -     |   26.11  \n   4    |   460   |   0.053258   |     -      |     -     |   26.05  \n   4    |   480   |   0.110463   |     -      |     -     |   26.08  \n   4    |   500   |   0.095672   |     -      |     -     |   26.18  \n   4    |   520   |   0.112931   |     -      |     -     |   26.24  \n   4    |   540   |   0.103869   |     -      |     -     |   26.14  \n   4    |   560   |   0.055404   |     -      |     -     |   26.10  \n   4    |   580   |   0.097936   |     -      |     -     |   26.10  \n   4    |   600   |   0.052078   |     -      |     -     |   26.16  \n   4    |   606   |   0.057331   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   4    |    -    |   0.081837   |  0.704816  |   84.81   |  825.62  \n----------------------------------------------------------------------\n\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   5    |   20    |   0.018114   |     -      |     -     |   27.46  \n   5    |   40    |   0.028991   |     -      |     -     |   26.10  \n   5    |   60    |   0.049602   |     -      |     -     |   26.07  \n   5    |   80    |   0.049008   |     -      |     -     |   26.05  \n   5    |   100   |   0.053812   |     -      |     -     |   26.03  \n   5    |   120   |   0.072620   |     -      |     -     |   26.08  \n   5    |   140   |   0.025770   |     -      |     -     |   26.10  \n   5    |   160   |   0.069165   |     -      |     -     |   26.11  \n   5    |   180   |   0.066999   |     -      |     -     |   26.09  \n   5    |   200   |   0.073383   |     -      |     -     |   26.10  \n   5    |   220   |   0.055811   |     -      |     -     |   26.10  \n   5    |   240   |   0.075456   |     -      |     -     |   26.09  \n   5    |   260   |   0.088140   |     -      |     -     |   26.07  \n   5    |   280   |   0.028008   |     -      |     -     |   26.10  \n   5    |   300   |   0.068750   |     -      |     -     |   26.15  \n   5    |   320   |   0.059598   |     -      |     -     |   26.15  \n   5    |   340   |   0.045083   |     -      |     -     |   26.16  \n   5    |   360   |   0.034492   |     -      |     -     |   26.20  \n   5    |   380   |   0.076379   |     -      |     -     |   26.18  \n   5    |   400   |   0.035995   |     -      |     -     |   26.20  \n   5    |   420   |   0.039004   |     -      |     -     |   26.17  \n   5    |   440   |   0.025841   |     -      |     -     |   26.19  \n   5    |   460   |   0.046406   |     -      |     -     |   26.06  \n   5    |   480   |   0.045192   |     -      |     -     |   26.12  \n   5    |   500   |   0.026545   |     -      |     -     |   26.24  \n   5    |   520   |   0.029535   |     -      |     -     |   26.22  \n   5    |   540   |   0.045082   |     -      |     -     |   26.23  \n   5    |   560   |   0.046840   |     -      |     -     |   26.17  \n   5    |   580   |   0.028203   |     -      |     -     |   26.12  \n   5    |   600   |   0.051803   |     -      |     -     |   26.11  \n   5    |   606   |   0.030778   |     -      |     -     |   7.70   \n----------------------------------------------------------------------\n   5    |    -    |   0.048427   |  0.730894  |   84.95   |  825.73  \n----------------------------------------------------------------------\n\n\nTraining complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"def bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"id":"ydTaXNbZgcGI","execution":{"iopub.status.busy":"2022-12-12T04:18:41.668291Z","iopub.execute_input":"2022-12-12T04:18:41.668902Z","iopub.status.idle":"2022-12-12T04:18:41.676243Z","shell.execute_reply.started":"2022-12-12T04:18:41.668863Z","shell.execute_reply":"2022-12-12T04:18:41.675118Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"torch.save(bert_classifier.state_dict(), \"/kaggle/working/modelv3.pt\")\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:18:47.428759Z","iopub.execute_input":"2022-12-12T04:18:47.429118Z","iopub.status.idle":"2022-12-12T04:18:48.670644Z","shell.execute_reply.started":"2022-12-12T04:18:47.429086Z","shell.execute_reply":"2022-12-12T04:18:48.669611Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = BertClassifier()\nmodel.load_state_dict(torch.load(\"/kaggle/working/modelv1.pt\"))\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"id":"7kF8Uz_dg1M-","execution":{"iopub.status.busy":"2022-12-12T04:19:13.244601Z","iopub.execute_input":"2022-12-12T04:19:13.244974Z","iopub.status.idle":"2022-12-12T04:19:13.253631Z","shell.execute_reply.started":"2022-12-12T04:19:13.244941Z","shell.execute_reply":"2022-12-12T04:19:13.252394Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"probs = bert_predict(bert_classifier, val_dataloader)\n\n# Evaluate the Bert classifier\nevaluate_roc(probs, Y_val.astype(float))","metadata":{"id":"tAcb4H35hv7r","outputId":"b0133587-6829-4790-f4c7-eca76d876a2d","execution":{"iopub.status.busy":"2022-12-12T04:19:18.681164Z","iopub.execute_input":"2022-12-12T04:19:18.682143Z","iopub.status.idle":"2022-12-12T04:19:52.071281Z","shell.execute_reply.started":"2022-12-12T04:19:18.682096Z","shell.execute_reply":"2022-12-12T04:19:52.070269Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"AUC: 0.9164\nAccuracy: 84.94%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/test-data/test-data.csv')\ntest_data.columns = ['Comment', 'Majority_Label']\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:20:02.277130Z","iopub.execute_input":"2022-12-12T04:20:02.278261Z","iopub.status.idle":"2022-12-12T04:20:02.315364Z","shell.execute_reply.started":"2022-12-12T04:20:02.278181Z","shell.execute_reply":"2022-12-12T04:20:02.314174Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                             Comment  Majority_Label\n0  Ø¨Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ Ø±ÙŠØ­ÙŠ Ø¬Ù…Ø§Ù„Ùƒ\\nÙˆØ®Ø·ÙŠ Ø§Ù„Ø®ÙˆØ¶ ÙÙŠ Ø§Ù„Ø´Ø±Ø¹\\nÙŠØ§...             NaN\n1  Ù„Ùˆ ØªØ±ÙŠØ¯ÙŠÙ† Ø§Ø®Ø° Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø±Ø£Ø© Ø®ÙˆØ°ÙŠÙ‡Ù… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØµØ­ Ù…...             NaN\n2                Ù†Ø§Ù‚ØµØ© Ø¹Ù‚Ù„ ÙˆØ¯ÙŠÙ† Ø£Ù†Øª ÙŠØ§ ØµØ§Ø­Ø¨Ø© Ø§Ù„Ù…Ù†Ø´ÙˆØ±             NaN\n3  Ø§ÙŠ Ø¹Ù…Ù„ Ù…Ù…ÙƒÙ† Ù„Ù„Ù…Ø±Ø£Ø© Ø§ØªÙ‚Ø§Ù†Ù‡ Ø¨Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ùˆ Ø¯Ù‚Ø© Ùˆ ...             NaN\n4  Ø§Ù„Ø´ÙŠ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ØªÙ…Ù‡Ø± ÙÙŠÙ‡ Ø§Ù„Ù…Ø±Ù±Ù‡ Ø¹Ù† Ø§Ù„Ø±Ø¬Ù„ Ù‡Ùˆ Ø§...             NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Majority_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ø¨Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ Ø±ÙŠØ­ÙŠ Ø¬Ù…Ø§Ù„Ùƒ\\nÙˆØ®Ø·ÙŠ Ø§Ù„Ø®ÙˆØ¶ ÙÙŠ Ø§Ù„Ø´Ø±Ø¹\\nÙŠØ§...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ù„Ùˆ ØªØ±ÙŠØ¯ÙŠÙ† Ø§Ø®Ø° Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø±Ø£Ø© Ø®ÙˆØ°ÙŠÙ‡Ù… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØµØ­ Ù…...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ù†Ø§Ù‚ØµØ© Ø¹Ù‚Ù„ ÙˆØ¯ÙŠÙ† Ø£Ù†Øª ÙŠØ§ ØµØ§Ø­Ø¨Ø© Ø§Ù„Ù…Ù†Ø´ÙˆØ±</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ø§ÙŠ Ø¹Ù…Ù„ Ù…Ù…ÙƒÙ† Ù„Ù„Ù…Ø±Ø£Ø© Ø§ØªÙ‚Ø§Ù†Ù‡ Ø¨Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ùˆ Ø¯Ù‚Ø© Ùˆ ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ø§Ù„Ø´ÙŠ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ØªÙ…Ù‡Ø± ÙÙŠÙ‡ Ø§Ù„Ù…Ø±Ù±Ù‡ Ø¹Ù† Ø§Ù„Ø±Ø¬Ù„ Ù‡Ùˆ Ø§...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.Comment = test_data.Comment.apply(cleaning_content)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:20:06.284127Z","iopub.execute_input":"2022-12-12T04:20:06.284904Z","iopub.status.idle":"2022-12-12T04:20:06.293934Z","shell.execute_reply.started":"2022-12-12T04:20:06.284865Z","shell.execute_reply":"2022-12-12T04:20:06.293013Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#model = model.to(device)\nmodel = bert_classifier   \ndef predict(review_text):\n    encoded_review = tokenizer.encode_plus(\n    review_text,\n    max_length=MAX_LEN,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    padding='longest',\n    return_attention_mask=True,\n    return_tensors='pt',\n    )\n\n    input_ids = encoded_review['input_ids'].to(device)\n    attention_mask = encoded_review['attention_mask'].to(device)\n    output = model(input_ids, attention_mask)\n    _, prediction = torch.max(output, dim=1)\n    #print(f'Review text: {review_text}')\n    index = output.cpu().data.numpy().argmax()\n    #print(f'Sentiment  : {index}')\n    return index\n\n\n\nresults = test_data.apply(lambda l: predict(l.Comment), axis=1)\n\ntest_data.Majority_Label = results\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:21:34.486471Z","iopub.execute_input":"2022-12-12T04:21:34.486895Z","iopub.status.idle":"2022-12-12T04:21:35.042203Z","shell.execute_reply.started":"2022-12-12T04:21:34.486862Z","shell.execute_reply":"2022-12-12T04:21:35.041268Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:37:30.199423Z","iopub.execute_input":"2022-12-12T04:37:30.200190Z","iopub.status.idle":"2022-12-12T04:37:30.212294Z","shell.execute_reply.started":"2022-12-12T04:37:30.200150Z","shell.execute_reply":"2022-12-12T04:37:30.211018Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                                              Comment  Majority_Label\n0   Ø¨Ø§Ù„Ù„Ù‡ Ø±ÙŠØ­ÙŠ Ø¬Ù…Ø§Ù„Ùƒ ÙˆØ®Ø·ÙŠ Ø§Ù„Ø®ÙˆØ¶ Ø§Ù„Ø´Ø±Ø¹ Ø¬Ùˆ ÙŠÙ‡Ù„Ù‡ ÙˆØ§Ù„Ù‡...               0\n1   ØªØ±ÙŠØ¯ÙŠÙ† Ø§Ø®Ø° Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø±Ø§Ù‡ Ø®ÙˆØ°ÙŠÙ‡Ù… Ø¨Ø·Ø±ÙŠÙ‚Ù‡ Ø§Ù„ØµØ­ Ù…Ø´ Ø¨...               1\n2                        Ù†Ø§Ù‚ØµÙ‡ Ø¹Ù‚Ù„ ÙˆØ¯ÙŠÙ† ØµØ§Ø­Ø¨Ù‡ Ø§Ù„Ù…Ù†Ø´ÙˆØ±               1\n3   Ø§ÙŠ Ø¹Ù…Ù„ Ù…Ù…ÙƒÙ† Ù„Ù„Ù…Ø±Ø§Ù‡ Ø§ØªÙ‚Ø§Ù†Ù‡ Ø¨Ø§ÙƒØ«Ø± ÙƒÙØ§Ø¡Ù‡ Ø¯Ù‚Ù‡ Ø¬ÙˆØ¯Ù‡...               0\n4   Ø§Ù„Ø´ÙŠ Ø§Ù„ÙˆØ­ÙŠØ¯ ØªÙ…Ù‡Ø± Ø§Ù„Ù…Ø±Ù±Ù‡ Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ÙƒÙˆØ¬ÙŠÙ†Ù‡ Øª...               0\n5   Ø§Ù„Ø¹Ø¬ÙŠØ¨ Ø§Ù…Ø±ÙƒÙ… ÙŠØªÙ‡Ø§ Ø§Ù„Ù†Ø³Ø§Ø¡ Ù„Ù…Ø§Ø°Ø§ ØªÙƒØ±ÙˆÙ‡Ù† Ø¬Ù†Ø³ÙƒÙ… ÙˆØª...               0\n6   ÙØ¹Ù„Ø§ Ø§Ù„Ø±Ø¬Ù„ Ø¹Ø§Ù„Ù‡ ÙˆÙ‚Øª ØªÙˆØ§ Ù†Ø³Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ù†Ø§Ø¬Ø­Ø§Øª Øª...               1\n7   Ø§Ù„Ø¨Ø§Ø·Ù„ Ø¨Ø¹ÙŠÙ†Ù‡ Ø§Ù„Ù…Ø±Ø§Ù‡ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¹ÙŠØ´ Ø¨Ø¯ÙˆÙ† Ø±Ø¬Ù„ ÙŠØ¹Ù†ÙŠ...               1\n8                         Ù…Ø§ÙÙŠØ´ Ø´ÙŠ Ø§Ù„Ø¯Ù†ÙŠØ§ Ø§Ø­Ù„ÙŠ Ø§Ù„Ù†Ø³Ø§Ø¡               0\n9                         Ù…Ø¬Ø±Ø¯ Ø¨Ø¨ØºØ§ÙˆØ§Øª ØªÙ†Ù‚Ù„ ÙˆØªÙ‚Ù„Ø¯ ÙÙ‚Ø·               0\n10                      Ø§Ù„Ø§Ù…Ù‡Ø§Øª ÙŠÙ…Ø§Ø±Ø³Ù† Ø§Ù„ Ø§ÙƒØ«Ø± Ø§Ù„Ø§Ø¨Ø§Ø¡               0\n11  Ø¨Ø¬Ø¯ Ø§Ù„Ù†Ø³ÙˆÙŠØ§Øª Ù†Ø§Ø³ Ù…Ø±ÙŠ Ø¨ØµØ­ÙŠØ­ Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¹Ø§ÙØ§Ù†ÙŠ Ø§Ø¨Øª...               0\n12  Ø¨Ø§Ù„Ù„Ù‡ Ø§Ù†Øª Ø¹Ù‚Ù„Ùƒ Ù…Ø¹Ø§Ùƒ Ù…Ø§Ø¬Ø±ØªÙŠÙ‡ Ø«ÙˆØ± Ù‚Ù„ÙŠÙ„Ù‡ Ø§Ù„Ø§Ø¯Ø¨ Ù‡Ø¯...               1\n13                        Ø§Ù„Ù…Ø±Ø§Ù‡ ÙØ§Ù„Ù…Ø·Ø¨Ø® Ø§Ø±Ø¶Ùˆ Ø¨Ø§Ù„ÙˆØ§Ù‚Ø¹               0\n14         Ø§Ø¹Ø±ÙÙŠ ØªÙƒÙ„Ù…ÙŠ Ø±ÙˆØ­ÙŠ Ø§Ø­Ø³Ù† Ù„ÙŠÙƒ ÙˆÙ„ÙŠÙ†Ø§ Ù„ÙŠØ¨ÙŠØ§ Ø±Ø¬Ø§Ù„               0\n15  ØªØ®ÙŠÙ„ÙˆØ§ Ø³ØªØªØ±Ø´Ø­ Ù„Ù„Ø¨Ø±Ù„Ù…Ø§Ù† ØºÙŠØ±Ù‡Ø§ Ø§Ù„ÙƒØ«ÙŠØ± ÙØ§Ø²Øª Ø¨Ø§Ù„Ø§Øµ...               0\n16  Ù…ØªØ¹Ø±ÙØ´ ØªÙ‚ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ† Ø¨Ø¹Ø¶Ù‡Ù… Ø¨Ø±ÙŠ Ø´Ø¯ÙŠ Ù…ÙƒØ§Ù†Ùƒ ÙˆÙ‚Ø¹Ù…Ø²ÙŠ Ø®ÙŠØ±               0\n17  Ø­ØªÙŠ Ø§Ù„Ù‚Ø±Ø§ÙŠÙ‡ Ø¹Ù†Ø¯Ù‡Ø§ Ø¨Ø§Ù„Ù‡ÙˆÙ† Ø§Ù„Ø§Ø®Øª Ø¨Ø±ÙŠ Ø³Ù„Ù… Ø¨Ù†ØªÙŠ Ø´Ø¯...               0\n18                Ø§Ù†ØªÙŠ Ù…Ø§Ù‚Ø¯Ø±ØªÙŠØ´ ØªÙ„Ù…ÙŠ Ø´Ø¹Ø±Ùƒ Ø¨ØªÙ„Ù…ÙŠ Ø§Ù„Ø´Ø¹Ø¨               1\n19       Ù…Ø´ Ù‡Ø¨ ÙˆØ¯Ø¨ Ø±Ø§Ù‡Ùˆ ØµØ­Ø¨Ø§ØªÙƒ Ø¹Ø·ÙˆÙƒ Ø§Ù„Ø­Ø§ÙØ² Ø±Ø§Ù‡Ù… ÙŠÙƒØ¯Ø¨Ùˆ               0\n20          Ù…Ø´ Ø¹Ø§Ø±ÙÙ‡ ØªØ¯ÙˆÙŠ ÙƒÙ„Ù…ØªÙŠÙ† ÙˆÙ…Ø±Ø´Ø­Ù‡ Ø±ÙˆØ­Ùƒ Ø´Ù† Ø¨Ù†ÙÙ‡Ù…               0\n21  Ø§Ù†Ø§ Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø±Ø§Ù‡ Ø´ÙŠ Ø§Ù„Ø§ Ø§Ù†Ù‡Ø§ ØªÙ…Ø³Ùƒ Ù„ÙŠØ¨ÙŠØ§ Ù„ÙŠØ¨ÙŠØ§ ÙŠ...               0\n22  Ø¨Ø¯Ù„ Ù…Ø§ØªØ±Ø´Ø­ Ø±ÙˆØ­Ù‡Ø§ ØªÙ…Ø´ÙŠ Ø¯ÙˆØ± Ù…Ø¯Ø±Ø³Ù‡ ØªÙ‚Ø±Ø§ ÙØ§Ù‡Ø§ ÙˆØªØªØ¹...               0\n23  Ø¨Ø±ÙŠ ÙƒÙ…Ù„ÙŠ Ù†ÙˆÙ…Ùƒ Ø®ÙŠØ±Ù„Ùƒ Ù†Ø§Ù‚ØµÙ†Ø§ Ø§Ù†ØªÙŠ ÙˆÙÙˆÙ‚ ØªÙ†ÙƒÙ„ Ø´Ù† Ø¨...               0\n24        Ø§Ø¯Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… ÙØ¶Ù‡ Ø§ÙˆÙ‚Ø§Øª ÙŠÙƒÙˆÙ† Ø³ÙƒÙˆØª ÙˆØ§Ø­Ø¯ÙŠØ³ÙƒØª Ø®ÙŠØ±               0\n25                         Ù…Ø¡Ù‡Ù„Ù‡ Ù†Ù‡Ø§Ø¡ÙŠØ§Ø§Ø§Ø§Ø§ Ø§Ù„Ù†ÙˆØ§Ø§Ø§Ø­ÙŠ               0\n26  Ø´Ø¹Ø±Ù‡Ø§ Ù…Ø´ Ù‚Ø§Ø¯Ø±Ù‡ ØªÙ„Ù…Ù‡ ØªØ¨ÙŠ ØªÙ„Ù… Ø§Ù„Ø´Ù…Ù„ ÙˆØªØ´Ø¯ Ù„ÙŠØ¨ÙŠØ§ Ùˆ...               0\n27  Ø§Ù„Ù…Ø¬Ø¯ Ù„Ù„Ù†Ø³Ø§Ø¡ Ø§Ù„Ù„Ø¨ÙŠØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ù„ÙŠØ¨ÙŠ Ù‚ØªÙ„ ÙˆÙ…Ø¹Ù†Ù Ùˆ...               1\n28  ÙŠØ§Ø±ÙˆØ­ÙŠ Ù‡Ø§Ø±Ø¨Ù‡ Ø¨Ù„Ø§Ø¯Ù‡Ø§ Ù„Ø¨Ù„Ø§Ø¯Ø§Øª Ù†Ø§Ø³ ÙŠØ¨Ùˆ Ø§Ù…Ø§Ù† Ø­Ø³Ø¨ÙŠ ...               1\n29  Ø±Ø¬Ø§Ù„ Ù‚ÙˆØ§Ù…ÙˆÙ† Ø¹Ù„ÙŠ Ù†Ø³Ø§Ø¡ ÙˆØ±Ø¬Ø§Ù„ Ø§Ø¹Ù„ÙŠ Ø¯Ø±Ø¬Ù‡ Ø§Ù„Ù„Ù‡ ÙƒØ±Ù… ...               0\n30                    Ù„Ø¹Ù†Ù‡ Ø§Ù„Ù„Ù‡ ÙŠØ§ÙƒØ§ÙØ±Ù‡ Ø§Ù„ÙŠ ÙŠÙˆÙ… Ø§Ù„Ø¯ÙŠÙ†               1\n31       Ø³ÙˆØ§Ø¡ Ø±Ø§Ø¬Ù„ Ø§Ùˆ Ù…Ø±Ø§Ù‡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ù…Ø§ÙŠØªÙ…Ù†Ø§Ø´ Ø§Ù„Ø´Ø± Ù„Ù„Ù†Ø§Ø³               1\n32  Ø§Ù„Ø¹Ù„Ù… Ø§Ù†Ù‡ Ù…Ø¹Ø¸Ù…Ù‡Ù… ÙŠÙ†Ø·Ø¨Ù‚Ø´ Ø¹Ù„ÙŠÙ‡Ù… Ù…ØµØ·Ù„Ø­ ÙˆÙ„ÙŠÙ‡ ÙŠÙ†Ø·Ø¨Ù‚...               0\n33  Ø§Ù„Ù†ÙˆØ¹ÙŠÙ‡ Ù‡Ø§Ø¯ÙŠ ØªÙ„Ù‚Ø§Ù‡Ø§ Ø®Ø§Ø´Ù‡ Ù„Ø¬Ù…Ø¹ÙŠÙ‡ Ø¨Ø§Ø´ ØªØ­ØµÙ„ Ø³ÙŠØ§Ø±Ù‡...               1\n34    ÙŠØ¯ÙŠØ±Ùˆ Ø±ÙˆØ­Ù‡Ù… Ù…Ø«Ù‚ÙØ§Øª Ø§Ù„Ù†ÙˆØ¹ ÙˆÙ‡Ù…Ø§ Ø§Ø®Ø±Ù‡Ù… Ø´Ù‡Ø§Ø¯Ù‡ Ù…ØªÙˆØ³Ø·               1\n35  Ù†Ø­ØªØ±Ù… Ø§Ù„Ø§Ù†Ø«ÙŠ Ø§Ù„Ù‚ÙˆÙŠÙ‡ Ù†ÙƒÙ†Ù„Ù‡Ø§ Ø§Ù„Ø§Ø­ØªØ±Ø§Ù… Ø­Ù‚ÙƒÙ… Ø§Ù„Ù…Ø³Ø§...               0\n36  ÙŠØ¯ÙˆØ±Ùˆ Ø§Ù„Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø³Ø§ÙˆØ§Ù‡ Ø§Ø¨Ø³Ø· Ù…Ø«Ø§Ù„ Ø§Ù„Ø®Ø¯Ù…Ù‡ Ø·Ù„Ø¨Ø§Øª Ø§Ù„...               0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Majority_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ø¨Ø§Ù„Ù„Ù‡ Ø±ÙŠØ­ÙŠ Ø¬Ù…Ø§Ù„Ùƒ ÙˆØ®Ø·ÙŠ Ø§Ù„Ø®ÙˆØ¶ Ø§Ù„Ø´Ø±Ø¹ Ø¬Ùˆ ÙŠÙ‡Ù„Ù‡ ÙˆØ§Ù„Ù‡...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ØªØ±ÙŠØ¯ÙŠÙ† Ø§Ø®Ø° Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø±Ø§Ù‡ Ø®ÙˆØ°ÙŠÙ‡Ù… Ø¨Ø·Ø±ÙŠÙ‚Ù‡ Ø§Ù„ØµØ­ Ù…Ø´ Ø¨...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ù†Ø§Ù‚ØµÙ‡ Ø¹Ù‚Ù„ ÙˆØ¯ÙŠÙ† ØµØ§Ø­Ø¨Ù‡ Ø§Ù„Ù…Ù†Ø´ÙˆØ±</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ø§ÙŠ Ø¹Ù…Ù„ Ù…Ù…ÙƒÙ† Ù„Ù„Ù…Ø±Ø§Ù‡ Ø§ØªÙ‚Ø§Ù†Ù‡ Ø¨Ø§ÙƒØ«Ø± ÙƒÙØ§Ø¡Ù‡ Ø¯Ù‚Ù‡ Ø¬ÙˆØ¯Ù‡...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ø§Ù„Ø´ÙŠ Ø§Ù„ÙˆØ­ÙŠØ¯ ØªÙ…Ù‡Ø± Ø§Ù„Ù…Ø±Ù±Ù‡ Ø§Ù„Ø±Ø¬Ù„ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ÙƒÙˆØ¬ÙŠÙ†Ù‡ Øª...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Ø§Ù„Ø¹Ø¬ÙŠØ¨ Ø§Ù…Ø±ÙƒÙ… ÙŠØªÙ‡Ø§ Ø§Ù„Ù†Ø³Ø§Ø¡ Ù„Ù…Ø§Ø°Ø§ ØªÙƒØ±ÙˆÙ‡Ù† Ø¬Ù†Ø³ÙƒÙ… ÙˆØª...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ÙØ¹Ù„Ø§ Ø§Ù„Ø±Ø¬Ù„ Ø¹Ø§Ù„Ù‡ ÙˆÙ‚Øª ØªÙˆØ§ Ù†Ø³Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ù†Ø§Ø¬Ø­Ø§Øª Øª...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Ø§Ù„Ø¨Ø§Ø·Ù„ Ø¨Ø¹ÙŠÙ†Ù‡ Ø§Ù„Ù…Ø±Ø§Ù‡ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø¹ÙŠØ´ Ø¨Ø¯ÙˆÙ† Ø±Ø¬Ù„ ÙŠØ¹Ù†ÙŠ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Ù…Ø§ÙÙŠØ´ Ø´ÙŠ Ø§Ù„Ø¯Ù†ÙŠØ§ Ø§Ø­Ù„ÙŠ Ø§Ù„Ù†Ø³Ø§Ø¡</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Ù…Ø¬Ø±Ø¯ Ø¨Ø¨ØºØ§ÙˆØ§Øª ØªÙ†Ù‚Ù„ ÙˆØªÙ‚Ù„Ø¯ ÙÙ‚Ø·</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Ø§Ù„Ø§Ù…Ù‡Ø§Øª ÙŠÙ…Ø§Ø±Ø³Ù† Ø§Ù„ Ø§ÙƒØ«Ø± Ø§Ù„Ø§Ø¨Ø§Ø¡</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Ø¨Ø¬Ø¯ Ø§Ù„Ù†Ø³ÙˆÙŠØ§Øª Ù†Ø§Ø³ Ù…Ø±ÙŠ Ø¨ØµØ­ÙŠØ­ Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¹Ø§ÙØ§Ù†ÙŠ Ø§Ø¨Øª...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Ø¨Ø§Ù„Ù„Ù‡ Ø§Ù†Øª Ø¹Ù‚Ù„Ùƒ Ù…Ø¹Ø§Ùƒ Ù…Ø§Ø¬Ø±ØªÙŠÙ‡ Ø«ÙˆØ± Ù‚Ù„ÙŠÙ„Ù‡ Ø§Ù„Ø§Ø¯Ø¨ Ù‡Ø¯...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Ø§Ù„Ù…Ø±Ø§Ù‡ ÙØ§Ù„Ù…Ø·Ø¨Ø® Ø§Ø±Ø¶Ùˆ Ø¨Ø§Ù„ÙˆØ§Ù‚Ø¹</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Ø§Ø¹Ø±ÙÙŠ ØªÙƒÙ„Ù…ÙŠ Ø±ÙˆØ­ÙŠ Ø§Ø­Ø³Ù† Ù„ÙŠÙƒ ÙˆÙ„ÙŠÙ†Ø§ Ù„ÙŠØ¨ÙŠØ§ Ø±Ø¬Ø§Ù„</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>ØªØ®ÙŠÙ„ÙˆØ§ Ø³ØªØªØ±Ø´Ø­ Ù„Ù„Ø¨Ø±Ù„Ù…Ø§Ù† ØºÙŠØ±Ù‡Ø§ Ø§Ù„ÙƒØ«ÙŠØ± ÙØ§Ø²Øª Ø¨Ø§Ù„Ø§Øµ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Ù…ØªØ¹Ø±ÙØ´ ØªÙ‚ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ† Ø¨Ø¹Ø¶Ù‡Ù… Ø¨Ø±ÙŠ Ø´Ø¯ÙŠ Ù…ÙƒØ§Ù†Ùƒ ÙˆÙ‚Ø¹Ù…Ø²ÙŠ Ø®ÙŠØ±</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Ø­ØªÙŠ Ø§Ù„Ù‚Ø±Ø§ÙŠÙ‡ Ø¹Ù†Ø¯Ù‡Ø§ Ø¨Ø§Ù„Ù‡ÙˆÙ† Ø§Ù„Ø§Ø®Øª Ø¨Ø±ÙŠ Ø³Ù„Ù… Ø¨Ù†ØªÙŠ Ø´Ø¯...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Ø§Ù†ØªÙŠ Ù…Ø§Ù‚Ø¯Ø±ØªÙŠØ´ ØªÙ„Ù…ÙŠ Ø´Ø¹Ø±Ùƒ Ø¨ØªÙ„Ù…ÙŠ Ø§Ù„Ø´Ø¹Ø¨</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Ù…Ø´ Ù‡Ø¨ ÙˆØ¯Ø¨ Ø±Ø§Ù‡Ùˆ ØµØ­Ø¨Ø§ØªÙƒ Ø¹Ø·ÙˆÙƒ Ø§Ù„Ø­Ø§ÙØ² Ø±Ø§Ù‡Ù… ÙŠÙƒØ¯Ø¨Ùˆ</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Ù…Ø´ Ø¹Ø§Ø±ÙÙ‡ ØªØ¯ÙˆÙŠ ÙƒÙ„Ù…ØªÙŠÙ† ÙˆÙ…Ø±Ø´Ø­Ù‡ Ø±ÙˆØ­Ùƒ Ø´Ù† Ø¨Ù†ÙÙ‡Ù…</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Ø§Ù†Ø§ Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø±Ø§Ù‡ Ø´ÙŠ Ø§Ù„Ø§ Ø§Ù†Ù‡Ø§ ØªÙ…Ø³Ùƒ Ù„ÙŠØ¨ÙŠØ§ Ù„ÙŠØ¨ÙŠØ§ ÙŠ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Ø¨Ø¯Ù„ Ù…Ø§ØªØ±Ø´Ø­ Ø±ÙˆØ­Ù‡Ø§ ØªÙ…Ø´ÙŠ Ø¯ÙˆØ± Ù…Ø¯Ø±Ø³Ù‡ ØªÙ‚Ø±Ø§ ÙØ§Ù‡Ø§ ÙˆØªØªØ¹...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Ø¨Ø±ÙŠ ÙƒÙ…Ù„ÙŠ Ù†ÙˆÙ…Ùƒ Ø®ÙŠØ±Ù„Ùƒ Ù†Ø§Ù‚ØµÙ†Ø§ Ø§Ù†ØªÙŠ ÙˆÙÙˆÙ‚ ØªÙ†ÙƒÙ„ Ø´Ù† Ø¨...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Ø§Ø¯Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… ÙØ¶Ù‡ Ø§ÙˆÙ‚Ø§Øª ÙŠÙƒÙˆÙ† Ø³ÙƒÙˆØª ÙˆØ§Ø­Ø¯ÙŠØ³ÙƒØª Ø®ÙŠØ±</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Ù…Ø¡Ù‡Ù„Ù‡ Ù†Ù‡Ø§Ø¡ÙŠØ§Ø§Ø§Ø§Ø§ Ø§Ù„Ù†ÙˆØ§Ø§Ø§Ø­ÙŠ</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Ø´Ø¹Ø±Ù‡Ø§ Ù…Ø´ Ù‚Ø§Ø¯Ø±Ù‡ ØªÙ„Ù…Ù‡ ØªØ¨ÙŠ ØªÙ„Ù… Ø§Ù„Ø´Ù…Ù„ ÙˆØªØ´Ø¯ Ù„ÙŠØ¨ÙŠØ§ Ùˆ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Ø§Ù„Ù…Ø¬Ø¯ Ù„Ù„Ù†Ø³Ø§Ø¡ Ø§Ù„Ù„Ø¨ÙŠØ¨Ø§Øª Ø§Ù„Ø°ÙƒØ± Ø§Ù„Ù„ÙŠØ¨ÙŠ Ù‚ØªÙ„ ÙˆÙ…Ø¹Ù†Ù Ùˆ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>ÙŠØ§Ø±ÙˆØ­ÙŠ Ù‡Ø§Ø±Ø¨Ù‡ Ø¨Ù„Ø§Ø¯Ù‡Ø§ Ù„Ø¨Ù„Ø§Ø¯Ø§Øª Ù†Ø§Ø³ ÙŠØ¨Ùˆ Ø§Ù…Ø§Ù† Ø­Ø³Ø¨ÙŠ ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Ø±Ø¬Ø§Ù„ Ù‚ÙˆØ§Ù…ÙˆÙ† Ø¹Ù„ÙŠ Ù†Ø³Ø§Ø¡ ÙˆØ±Ø¬Ø§Ù„ Ø§Ø¹Ù„ÙŠ Ø¯Ø±Ø¬Ù‡ Ø§Ù„Ù„Ù‡ ÙƒØ±Ù… ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Ù„Ø¹Ù†Ù‡ Ø§Ù„Ù„Ù‡ ÙŠØ§ÙƒØ§ÙØ±Ù‡ Ø§Ù„ÙŠ ÙŠÙˆÙ… Ø§Ù„Ø¯ÙŠÙ†</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Ø³ÙˆØ§Ø¡ Ø±Ø§Ø¬Ù„ Ø§Ùˆ Ù…Ø±Ø§Ù‡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ù…Ø§ÙŠØªÙ…Ù†Ø§Ø´ Ø§Ù„Ø´Ø± Ù„Ù„Ù†Ø§Ø³</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Ø§Ù„Ø¹Ù„Ù… Ø§Ù†Ù‡ Ù…Ø¹Ø¸Ù…Ù‡Ù… ÙŠÙ†Ø·Ø¨Ù‚Ø´ Ø¹Ù„ÙŠÙ‡Ù… Ù…ØµØ·Ù„Ø­ ÙˆÙ„ÙŠÙ‡ ÙŠÙ†Ø·Ø¨Ù‚...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Ø§Ù„Ù†ÙˆØ¹ÙŠÙ‡ Ù‡Ø§Ø¯ÙŠ ØªÙ„Ù‚Ø§Ù‡Ø§ Ø®Ø§Ø´Ù‡ Ù„Ø¬Ù…Ø¹ÙŠÙ‡ Ø¨Ø§Ø´ ØªØ­ØµÙ„ Ø³ÙŠØ§Ø±Ù‡...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>ÙŠØ¯ÙŠØ±Ùˆ Ø±ÙˆØ­Ù‡Ù… Ù…Ø«Ù‚ÙØ§Øª Ø§Ù„Ù†ÙˆØ¹ ÙˆÙ‡Ù…Ø§ Ø§Ø®Ø±Ù‡Ù… Ø´Ù‡Ø§Ø¯Ù‡ Ù…ØªÙˆØ³Ø·</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Ù†Ø­ØªØ±Ù… Ø§Ù„Ø§Ù†Ø«ÙŠ Ø§Ù„Ù‚ÙˆÙŠÙ‡ Ù†ÙƒÙ†Ù„Ù‡Ø§ Ø§Ù„Ø§Ø­ØªØ±Ø§Ù… Ø­Ù‚ÙƒÙ… Ø§Ù„Ù…Ø³Ø§...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>ÙŠØ¯ÙˆØ±Ùˆ Ø§Ù„Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø³Ø§ÙˆØ§Ù‡ Ø§Ø¨Ø³Ø· Ù…Ø«Ø§Ù„ Ø§Ù„Ø®Ø¯Ù…Ù‡ Ø·Ù„Ø¨Ø§Øª Ø§Ù„...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data.to_csv('/kaggle/working/test_result.csv', encoding='utf-16', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T04:37:51.919812Z","iopub.execute_input":"2022-12-12T04:37:51.920200Z","iopub.status.idle":"2022-12-12T04:37:51.933798Z","shell.execute_reply.started":"2022-12-12T04:37:51.920167Z","shell.execute_reply":"2022-12-12T04:37:51.932743Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}